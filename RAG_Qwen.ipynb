{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14168970,"sourceType":"datasetVersion","datasetId":8847293}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project 2 — RAG application (LangChain + Chroma + HF LLM)\n\n### 1. Présentation \nCette cellule sert d'introduction au projet. Elle définit les composants clés :\n* **Modèle (LLM) :** Qwen2.5-7B-Instruct (Performance proche de GPT-3.5/4 mais open-source).\n* **Framework :** LangChain (pour connecter les données au modèle).\n* **Base de données :** ChromaDB (pour stocker la mémoire sémantique des cours).\n","metadata":{}},{"cell_type":"markdown","source":"### 2. Installation des dépendances\n**Commande :** `!pip -q install ...`\nCette étape installe les bibliothèques Python nécessaires dans l'environnement (Kaggle/Colab) :\n* `langchain` & `langchain-community` : Outils pour construire le RAG.\n* `chromadb` : La base de données vectorielle.\n* `transformers` & `accelerate` : Pour télécharger et exécuter le modèle Qwen depuis Hugging Face.\n* `pypdf`, `python-docx` : Pour extraire le texte des fichiers de cours.","metadata":{}},{"cell_type":"code","source":"!pip -q install -U \\\n  langchain langchain-community langchain-core langchain-text-splitters \\\n  langchain-huggingface chromadb \\\n  sentence-transformers transformers accelerate \\\n  pypdf python-docx\n\nprint(\"✅ Install OK\")","metadata":{"execution":{"iopub.status.busy":"2025-12-16T12:18:50.279728Z","iopub.execute_input":"2025-12-16T12:18:50.280025Z","iopub.status.idle":"2025-12-16T12:18:55.453980Z","shell.execute_reply.started":"2025-12-16T12:18:50.280002Z","shell.execute_reply":"2025-12-16T12:18:55.453104Z"},"trusted":true},"outputs":[{"name":"stdout","text":"✅ Install OK\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### 3. Importations\n**Contenu :** `import os`, `from langchain...`\nChargement des modules nécessaires.\n* **Gestion d'erreurs :** Le code utilise des blocs `try/except` pour gérer les différences de versions de LangChain (entre `langchain_huggingface` et `langchain_community`), assurant que le code ne plante pas si l'environnement change.","metadata":{}},{"cell_type":"code","source":"import os\nfrom typing import List\nfrom pypdf import PdfReader\nimport docx\n\n# LangChain imports (robustes selon versions)\nfrom langchain_core.documents import Document\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Embeddings HF\ntry:\n    from langchain_huggingface import HuggingFaceEmbeddings\nexcept Exception:\n    # fallback older versions\n    from langchain_community.embeddings import HuggingFaceEmbeddings\n\n# Chroma vector store\ntry:\n    from langchain_community.vectorstores import Chroma\nexcept Exception:\n    from langchain_chroma import Chroma  \n# LLM wrapper\ntry:\n    from langchain_huggingface import HuggingFacePipeline\nexcept Exception:\n    from langchain_community.llms import HuggingFacePipeline\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\n\nprint(\"✅ Imports OK\")","metadata":{"execution":{"iopub.status.busy":"2025-12-16T12:18:55.455831Z","iopub.execute_input":"2025-12-16T12:18:55.456131Z","iopub.status.idle":"2025-12-16T12:19:11.230774Z","shell.execute_reply.started":"2025-12-16T12:18:55.456096Z","shell.execute_reply":"2025-12-16T12:19:11.230103Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-12-16 12:19:02.775140: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765887542.797228     352 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765887542.803713     352 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"✅ Imports OK\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### 4. Ingestion des Données (`load_course_materials`)\nCette fonction est responsable de la lecture des fichiers sources.\n1.  **Parcours :** Elle scanne le dossier `DATA_DIR` récursivement.\n2.  **Extraction :** Elle détecte le format (.pdf, .docx, .txt) et extrait le texte brut.\n3.  **Métadonnées :** Chaque document est tagué avec son nom de fichier et le numéro de page (pour les PDF), ce qui permettra de citer les sources précises dans la réponse finale.","metadata":{}},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/mini-data/data\"   \ndef load_course_materials(path: str) -> List[Document]:\n    docs: List[Document] = []\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Dossier introuvable: {path}\")\n\n    for root, _, files in os.walk(path):\n        for file in files:\n            full = os.path.join(root, file)\n            lower = file.lower()\n\n            if lower.endswith(\".pdf\"):\n                try:\n                    reader = PdfReader(full)\n                    for page_idx, page in enumerate(reader.pages):\n                        text = page.extract_text() or \"\"\n                        text = text.strip()\n                        if not text:\n                            continue\n                        docs.append(Document(\n                            page_content=text,\n                            metadata={\"source\": full, \"filetype\": \"pdf\", \"page\": page_idx + 1}\n                        ))\n                except Exception as e:\n                    print(f\"⚠️ PDF erreur: {full} -> {e}\")\n\n            elif lower.endswith(\".txt\"):\n                try:\n                    with open(full, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                        text = f.read().strip()\n                    if text:\n                        docs.append(Document(\n                            page_content=text,\n                            metadata={\"source\": full, \"filetype\": \"txt\"}\n                        ))\n                except Exception as e:\n                    print(f\"⚠️ TXT erreur: {full} -> {e}\")\n\n            elif lower.endswith(\".docx\"):\n                try:\n                    d = docx.Document(full)\n                    text = \"\\n\".join([p.text for p in d.paragraphs]).strip()\n                    if text:\n                        docs.append(Document(\n                            page_content=text,\n                            metadata={\"source\": full, \"filetype\": \"docx\"}\n                        ))\n                except Exception as e:\n                    print(f\"⚠️ DOCX erreur: {full} -> {e}\")\n\n    return docs\n\ndocs_raw = load_course_materials(DATA_DIR)\nprint(f\"✅ Documents chargés: {len(docs_raw)}\")\n\n# aperçu\nif docs_raw:\n    print(\"Exemple metadata:\", docs_raw[0].metadata)\n    print(\"Extrait:\", docs_raw[0].page_content[:300])","metadata":{"execution":{"iopub.status.busy":"2025-12-16T12:19:11.231457Z","iopub.execute_input":"2025-12-16T12:19:11.232000Z","iopub.status.idle":"2025-12-16T12:19:18.857471Z","shell.execute_reply.started":"2025-12-16T12:19:11.231973Z","shell.execute_reply":"2025-12-16T12:19:18.856810Z"},"trusted":true},"outputs":[{"name":"stdout","text":"✅ Documents chargés: 621\nExemple metadata: {'source': '/kaggle/input/mini-data/data/CH1_Big Data Analytics.pdf', 'filetype': 'pdf', 'page': 1}\nExtrait: Master Big Data Analytics & Smart Systems (BDSaS)\nBig Data Analytics II\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### 5. Découpage (Chunking)\n**Outil :** `RecursiveCharacterTextSplitter`\nLes LLM ont une limite de mémoire (fenêtre de contexte). On ne peut pas leur envoyer tout le cours d'un coup.\n* **Chunk Size (900) :** On découpe le texte en blocs de 900 caractères.\n* **Overlap (150) :** On garde une superposition de 150 caractères entre les blocs pour ne pas couper une phrase importante au milieu et conserver le contexte.","metadata":{}},{"cell_type":"code","source":"splitter = RecursiveCharacterTextSplitter(\n    chunk_size=900,\n    chunk_overlap=150,\n)\n\ndocs = splitter.split_documents(docs_raw)\nprint(\"✅ Chunks:\", len(docs))\n\n# aperçu d'un chunk\nif docs:\n    print(\"Exemple chunk metadata:\", docs[0].metadata)\n    print(\"Extrait chunk:\", docs[0].page_content[:250])","metadata":{"execution":{"iopub.status.busy":"2025-12-16T12:19:18.858265Z","iopub.execute_input":"2025-12-16T12:19:18.858591Z","iopub.status.idle":"2025-12-16T12:19:18.878585Z","shell.execute_reply.started":"2025-12-16T12:19:18.858570Z","shell.execute_reply":"2025-12-16T12:19:18.877736Z"},"trusted":true},"outputs":[{"name":"stdout","text":"✅ Chunks: 689\nExemple chunk metadata: {'source': '/kaggle/input/mini-data/data/CH1_Big Data Analytics.pdf', 'filetype': 'pdf', 'page': 1}\nExtrait chunk: Master Big Data Analytics & Smart Systems (BDSaS)\nBig Data Analytics II\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### 6. Indexation Vectorielle (Embeddings & Chroma)\nCette étape transforme le texte en mathématiques pour la recherche.\n1.  **Embeddings (`all-MiniLM-L6-v2`) :** Ce petit modèle transforme les phrases en vecteurs numériques (listes de nombres).\n2.  **Vector Store (Chroma) :** Les vecteurs sont stockés dans ChromaDB.\n3.  **Persistance :** La base est sauvegardée sur le disque (`persist_directory`) pour éviter de tout recalculer à chaque fois.\n4.  **Retriever :** La base est convertie en moteur de recherche configuré pour renvoyer les **5 passages** les plus pertinents (`k=5`) pour chaque question.","metadata":{}},{"cell_type":"code","source":"EMB_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\nembeddings = HuggingFaceEmbeddings(model_name=EMB_MODEL)\nPERSIST_DIR = \"./chroma_db_project2\"\nvectordb = Chroma.from_documents(\n    documents=docs,\n    embedding=embeddings,\n    persist_directory=PERSIST_DIR,\n)\ntry:\n    vectordb.persist()\nexcept Exception:\n    pass\n\nretriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n\nprint(\"✅ ChromaDB prêt (persist_directory =\", PERSIST_DIR, \")\")","metadata":{"execution":{"iopub.status.busy":"2025-12-16T12:19:18.880664Z","iopub.execute_input":"2025-12-16T12:19:18.880914Z","iopub.status.idle":"2025-12-16T12:19:26.077148Z","shell.execute_reply.started":"2025-12-16T12:19:18.880890Z","shell.execute_reply":"2025-12-16T12:19:26.076383Z"},"trusted":true},"outputs":[{"name":"stdout","text":"✅ ChromaDB prêt (persist_directory = ./chroma_db_project2 )\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_352/1969108273.py:10: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n  vectordb.persist()\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### 7. Chargement du LLM (Qwen)\n**Modèle :** `Qwen/Qwen2.5-7B-Instruct`\n* Le modèle est chargé en mémoire (GPU si disponible via `device_map=\"auto\"`).\n* **Pipeline :** On configure la génération de texte (ex: `temperature`, `max_new_tokens`) pour avoir des réponses précises et non créatives (idéal pour des cours).","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"  \n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n    trust_remote_code=True,\n)\n\ngen_pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    do_sample=False,\n    temperature=None,\n    max_new_tokens=300,\n    return_full_text=False,\n)\n\nllm = HuggingFacePipeline(pipeline=gen_pipe)\n\nprint(\"✅ LLM prêt:\", MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2025-12-16T12:21:20.058919Z","iopub.execute_input":"2025-12-16T12:21:20.059489Z","iopub.status.idle":"2025-12-16T12:21:39.348024Z","shell.execute_reply.started":"2025-12-16T12:21:20.059461Z","shell.execute_reply":"2025-12-16T12:21:39.347476Z"},"trusted":true},"outputs":[{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d45593ab9d344bcb152af1aaa002365"}},"metadata":{}},{"name":"stderr","text":"WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\nDevice set to use cuda:0\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"✅ LLM prêt: Qwen/Qwen2.5-7B-Instruct\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### 8. Construction de la Chaîne RAG\nC'est le cerveau de l'application qui relie tout.\n1.  **Prompt Engineering :** Le `SYSTEM_MSG` donne des ordres stricts au modèle (\"Tu es un assistant utile...\", \"Utilise uniquement le contexte fourni\").\n2.  **Formatage :** La fonction `format_docs` prépare les extraits de cours trouvés en ajoutant `[SOURCE: ...]` devant.\n3.  **Template Chat :** On utilise le format spécifique attendu par Qwen (`<|im_start|>system...`) pour qu'il comprenne bien son rôle.\n4.  **La Chaîne (LCEL) :** La syntaxe `retriever | format | prompt | llm` crée le flux automatique : *Question -> Recherche -> Prompt -> Génération*.","metadata":{}},{"cell_type":"code","source":"from typing import List\nfrom langchain_core.documents import Document\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Message système (string multi-lignes correcte)\nSYSTEM_MSG = \"\"\"Tu es un assistant qui répond UNIQUEMENT à partir du contexte.\n\nRÈGLES :\n- Si la réponse n'est pas dans le contexte → réponds EXACTEMENT :\n  \"Je ne sais pas, ce n'est pas indiqué dans les documents.\"\n- Ne fais AUCUNE déduction.\n- Réponds toujours en français.\n\"\"\"\n\ndef format_docs(docs: List[Document]) -> str:\n    \"\"\"Concatène les documents récupérés avec leurs sources\"\"\"\n    blocks = []\n    for d in docs:\n        src = d.metadata.get(\"source\", \"unknown\")\n        page = d.metadata.get(\"page\", None)\n\n        if page is not None:\n            header = f\"[SOURCE: {src} | page {page}]\"\n        else:\n            header = f\"[SOURCE: {src}]\"\n\n        blocks.append(header + \"\\n\" + d.page_content)\n\n    return \"\\n\\n\".join(blocks)\n\ndef build_qwen_prompt(inputs: dict) -> str:\n    \"\"\"Construit le prompt compatible avec le chat template Qwen\"\"\"\n    context = inputs[\"context\"]\n    question = inputs[\"question\"]\n\n    user_msg = (\n        f\"Contexte :\\n{context}\\n\\n\"\n        f\"Question :\\n{question}\\n\\n\"\n        f\"Réponse :\"\n    )\n\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n        {\"role\": \"user\", \"content\": user_msg},\n    ]\n\n    return tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n\n# Chaîne RAG LangChain (LCEL)\nrag_chain = (\n    {\n        \"context\": retriever | format_docs,\n        \"question\": RunnablePassthrough(),\n    }\n    | RunnableLambda(build_qwen_prompt)\n    | llm\n    | StrOutputParser()\n)\n\nprint(\"✅ RAG chain prête\")\n","metadata":{"execution":{"iopub.status.busy":"2025-12-16T12:21:48.994917Z","iopub.execute_input":"2025-12-16T12:21:48.995606Z","iopub.status.idle":"2025-12-16T12:21:49.003471Z","shell.execute_reply.started":"2025-12-16T12:21:48.995580Z","shell.execute_reply":"2025-12-16T12:21:49.002534Z"},"trusted":true},"outputs":[{"name":"stdout","text":"✅ RAG chain prête\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### 9. Exécution et Test\n**Fonctions :** `ask(question)` et `ask1(question)`\n* Ces fonctions envoient une question à la chaîne RAG.\n* `ask` : Affiche la réponse + les sources détaillées (nom du fichier, page).\n* `ask1` : Affiche uniquement la réponse générée.","metadata":{}},{"cell_type":"code","source":"def ask(question: str, k: int = 5):\n    try:\n        docs = retriever.get_relevant_documents(question)\n    except Exception:\n        docs = retriever.invoke(question)\n\n    answer = rag_chain.invoke(question)\n\n    print(\"QUESTION:\", question)\n    print(\"\\nRÉPONSE:\\n\", answer)\n\n    print(\"\\nSOURCES (top-k):\")\n    for i, d in enumerate(docs[:k], 1):\n        src = d.metadata.get(\"source\", \"unknown\")\n        page = d.metadata.get(\"page\", \"\")\n        if page:\n            print(f\"{i}. {src} (page {page})\")\n        else:\n            print(f\"{i}. {src}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T12:21:59.463777Z","iopub.execute_input":"2025-12-16T12:21:59.464488Z","iopub.status.idle":"2025-12-16T12:21:59.469473Z","shell.execute_reply.started":"2025-12-16T12:21:59.464459Z","shell.execute_reply":"2025-12-16T12:21:59.468677Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def ask1(question: str):\n    answer = rag_chain.invoke(question)\n\n    print(\"QUESTION:\", question)\n    print(\"\\nRÉPONSE:\\n\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T12:22:49.521352Z","iopub.execute_input":"2025-12-16T12:22:49.522244Z","iopub.status.idle":"2025-12-16T12:22:49.526153Z","shell.execute_reply.started":"2025-12-16T12:22:49.522213Z","shell.execute_reply":"2025-12-16T12:22:49.525243Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"ask(\"What is Memory-based Collaborative Filtering ?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T12:25:46.314790Z","iopub.execute_input":"2025-12-16T12:25:46.315130Z","iopub.status.idle":"2025-12-16T12:27:17.591181Z","shell.execute_reply.started":"2025-12-16T12:25:46.315104Z","shell.execute_reply":"2025-12-16T12:27:17.590376Z"}},"outputs":[{"name":"stdout","text":"QUESTION: What is Memory-based Collaborative Filtering ?\n\nRÉPONSE:\n Memory-based Collaborative Filtering, également appelé l'approche de voisinage, utilise directement la base de données entière d'utilisateurs et d'items (la matrice d'utilité) pour générer des prédictions, sans construire de modèle. Cette approche inclut à la fois les algorithmes basés sur les utilisateurs et ceux basés sur les items. En essentiel, un tel algorithme trouve explicitement des utilisateurs ou des items similaires (appelés voisins) et utilise ces derniers pour prédire les préférences de l'utilisateur cible. Il utilise des classifieurs de voisins les plus proches pour prédire les notes des utilisateurs ou leur propension à acheter en mesurant la corrélation entre le profil de l'utilisateur cible (qui peut être un ensemble de notes d'items ou un ensemble d'items visités ou achetés) et les profils d'autres utilisateurs.\n\nSOURCES (top-k):\n1. /kaggle/input/mini-data/data/3 Memory-based Collaborative Filtering-3-1-2 (1).pdf (page 1)\n2. /kaggle/input/mini-data/data/3 Memory-based Collaborative Filtering-3-1-2 (1).pdf (page 1)\n3. /kaggle/input/mini-data/data/3 Memory-based Collaborative Filtering-3-1-2 (1).pdf (page 2)\n4. /kaggle/input/mini-data/data/3 Memory-based Collaborative Filtering-3-1-2 (1).pdf (page 2)\n5. /kaggle/input/mini-data/data/3 Memory-based Collaborative Filtering-3-1-2 (1).pdf (page 19)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"ask1(\"What is Bitcoin ?\")","metadata":{"execution":{"iopub.status.busy":"2025-12-16T12:22:57.087733Z","iopub.execute_input":"2025-12-16T12:22:57.088230Z","iopub.status.idle":"2025-12-16T12:24:16.040728Z","shell.execute_reply.started":"2025-12-16T12:22:57.088207Z","shell.execute_reply":"2025-12-16T12:24:16.039998Z"},"trusted":true},"outputs":[{"name":"stdout","text":"QUESTION: What is Bitcoin ?\n\nRÉPONSE:\n Bitcoin est le nom d'un protocole, d'un réseau peer-to-peer et d'une innovation informatique distribuée. C'est également la première application d'une invention qui représente l'aboutissement de décennies de recherche en cryptographie et en systèmes distribués. Il s'agit d'un ensemble de concepts et de technologies constituant la base d'un écosystème de monnaie numérique, comprenant quatre innovations clés : un réseau peer-to-peer décentralisé (le protocole Bitcoin), un registre public des transactions (la blockchain), un ensemble de règles pour la validation indépendante des transactions et l'émission de devises (règles de consensus), et un mécanisme pour parvenir à un consensus décentralisé mondial sur la blockchain valide (algorithme Proof-of-Work).\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}